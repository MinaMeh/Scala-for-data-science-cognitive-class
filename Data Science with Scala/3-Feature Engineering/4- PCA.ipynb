{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "# Module 3: Feature Engineering\n",
    "\n",
    "## Principal Component Analysis (PCA) in Feature Englineering\n",
    "\n",
    "### Lesson Objectives\n",
    "\n",
    "After completing this lesson, you should be able to: \n",
    "\n",
    "- Understand what Principal Component Analysis (PCA) is\n",
    "-\tUnderstand PCA's role in feature engineering \n",
    "\n",
    "\n",
    "\n",
    "## PCA: Definition \n",
    "\n",
    "-\tPCA is a dimension reduction technique. it is unsupervised machine learning, and it has many uses; on this video we only care about its use for feature engineering\n",
    "\n",
    "\n",
    "## PCA: How It Works\n",
    "\n",
    "-\tThe first Principal Component (PC) is defined as the linear combination of the predictors that captures the most variability of all possible linear combinations.\n",
    "-\tThen, subsequent PCs are derived such that these linear combinations capture the most remaining variability while also being uncorrelated with all previous PCs.\n",
    "\n",
    "\n",
    "## Feature Engineering \n",
    "\n",
    "-\t\"Feature Engineering\" is a practice where predictors are created and refined to maximize model performance\n",
    "-\tIt can take quite some time to identify and prepare relevant features\n",
    "\n",
    "\n",
    "## Feature Engineering with PCA\n",
    "\n",
    "-\tBasic idea: generate a smaller set of variables that capture most of the information in the original variables\n",
    "-\tThe new predictors are functions of the original predictors; all the original predictors are still needed to create the surrogate variables Dataset: Predict US Crimes\n",
    "-\tWe want to predict the proportion of violent crimes per 100k population on different locations in the US\n",
    "-\tMore than 100 predictors. Examples:\n",
    "  -\t`householdsize`: mean people per household\n",
    "  - `PctLess9thGrade`: percentage of offenders who have not yet entered high school\n",
    "  - `pctWWage`: percentage of households with wage or salary income in 1989\n",
    "-\tFor a description of these variables, see the UCI repository (communities and crimes) Dataset: Predict US Crimes \n",
    "-\tLet's assume that we don't want to operate with those >100 predictors. Why?\n",
    "-\tSome will be collinear (ie highly correlated)\n",
    "- It's hard to see relationships in a high-dimensional space \n",
    "-\tHow do we use PCA to get down to 10 dimensions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.SparkContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@7499589"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.4.0` // Or use any other 2.x version here\n",
    "import  org.apache.spark.SparkContext\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "val sc= new SparkContext(\"local[*]\",\"PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2019-12-19 12:40:10--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0105EN/data/UScrime2-colsLotsOfNAremoved.csv\n",
      "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
      "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 971758 (949K) [text/csv]\n",
      "Saving to: ‘UScrime2-colsLotsOfNAremoved.csv’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  5%  185K 5s\n",
      "    50K .......... .......... .......... .......... .......... 10% 1.01M 3s\n",
      "   100K .......... .......... .......... .......... .......... 15%  455K 2s\n",
      "   150K .......... .......... .......... .......... .......... 21% 1.04M 2s\n",
      "   200K .......... .......... .......... .......... .......... 26% 1.04M 1s\n",
      "   250K .......... .......... .......... .......... .......... 31% 1.05M 1s\n",
      "   300K .......... .......... .......... .......... .......... 36% 1.06M 1s\n",
      "   350K .......... .......... .......... .......... .......... 42%  965K 1s\n",
      "   400K .......... .......... .......... .......... .......... 47%  874K 1s\n",
      "   450K .......... .......... .......... .......... .......... 52% 21.7K 3s\n",
      "   500K .......... .......... .......... .......... .......... 57% 63.4K 3s\n",
      "   550K .......... .......... .......... .......... .......... 63%  718K 2s\n",
      "   600K .......... .......... .......... .......... .......... 68%  507K 2s\n",
      "   650K .......... .......... .......... .......... .......... 73% 75.0K 2s\n",
      "   700K .......... .......... .......... .......... .......... 79% 57.1K 1s\n",
      "   750K .......... .......... .......... .......... .......... 84% 97.2K 1s\n",
      "   800K .......... .......... .......... .......... .......... 89%  248K 1s\n",
      "   850K .......... .......... .......... .......... .......... 94% 8.09M 0s\n",
      "   900K .......... .......... .......... .......... ........  100%  136K=6.6s\n",
      "\n",
      "2019-12-19 12:40:18 (143 KB/s) - ‘UScrime2-colsLotsOfNAremoved.csv’ saved [971758/971758]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msys.process._\n",
       "\u001b[39m\n",
       "\u001b[36mres1_1\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m0\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\"wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0105EN/data/UScrime2-colsLotsOfNAremoved.csv \" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@65b9e8fa\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.linalg.Vectors\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.feature.{VectorAssembler, PCA}\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "import spark.implicits._\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, PCA}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcrimes\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [community: string, population: double ... 99 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val crimes = spark.read.\n",
    "    format(\"com.databricks.spark.csv\").\n",
    "    option(\"delimiter\", \",\").\n",
    "    option(\"header\", \"true\").\n",
    "    option(\"inferSchema\", \"true\").\n",
    "    load(\"UScrime2-colsLotsOfNAremoved.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/12/19 12:43:32 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "19/12/19 12:43:32 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "19/12/19 12:43:34 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "19/12/19 12:43:34 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|pcaFeatures                                                                                                                                                                                              |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[1.2138889196984592,0.5645677593367362,-0.022284837106876964,-0.3093435457824009,-0.8195107213520451,-0.42466623894186817,-0.41000046615093594,0.6418064693954159,0.19967782074511403,0.507495196949215] |\n",
      "|[0.6279851901950964,1.1668941486566553,-0.5141643066488679,-0.2483413354182063,-0.624463455060983,-0.11825739720679951,-0.4505024369417832,1.2455920970597112,0.02530068973793365,0.6837817089267021]    |\n",
      "|[0.2343490431890697,0.3480701442283821,0.5487688416045003,-0.4820705759469181,-0.4084800597147229,-0.9437371849824483,0.11599639720667394,1.0367345481853705,-0.04725240892400964,0.20826607196982544]   |\n",
      "|[1.4380837465069716,1.1668939029365468,0.9338754897868397,-0.7461113718889459,-1.483139947075772,-1.5778331168936692,-0.20616686268679388,0.8902551111098841,-0.5251717996202457,0.32072526019465486]    |\n",
      "|[1.906331791609544,0.17241550091727154,1.070769758992437,-0.5156160636918886,-0.6947184007375663,-0.716175388023182,-0.6534620035016484,0.6081667003612188,-0.05439131706575517,0.642061448808755]       |\n",
      "|[2.196544145813238,2.295805083432602,-0.16479628335321025,-0.6184203551609575,0.21393637000194468,-0.8291640239379829,-0.48507954786491436,0.8215618273002672,0.058352818030754615,0.6094345898658373]   |\n",
      "|[1.6194543817001257,0.12095691679237029,0.8257591521802178,-0.9892508993504449,-0.20870877015319583,-0.8042139097270437,-0.7541391214616039,0.5988458947159307,-0.0014156043714952254,0.4590255053368099]|\n",
      "|[-0.8453821810000567,2.441305303767359,1.8889826265731477,-0.008573687459836533,0.20904347817113894,-0.8029822218874751,-0.5312186260152006,0.5881344788372522,-0.0013195553614700271,0.8821138644258724]|\n",
      "|[0.048076956841223026,0.06827975003700423,0.4430013799685326,-0.8385221699178894,-0.6366841874833639,-0.3846396703366199,-0.6201340897321547,0.7207656290452404,-0.09979032997396037,0.49245008397196915]|\n",
      "|[2.1298913291536175,1.093882881285353,-0.7690865716278297,0.09305469847421864,0.08265471634975716,-1.2366466840254404,-1.1331835884988508,0.45902666331786357,-0.1408842043770599,0.31087202925428736]   |\n",
      "|[1.2619453873232407,3.7136169425865266,0.8785484296971551,-0.5795833118090383,-0.2437880134599484,-0.6485131218917433,-0.6783025112145797,1.0007653933196679,0.058020623831884555,0.34427252676833986]   |\n",
      "|[2.3710331353270084,0.8974881456336445,0.5357494727172735,-1.3553785398899214,-0.02541123043338041,-1.5332010982716615,-1.0687266145265144,0.5557543728950708,0.03424714218392922,0.41177219211672045]   |\n",
      "|[2.8546728351155197,0.8822124966494863,0.3784791624769007,-0.9293604771778787,-0.08076165446216317,-1.4408714923702888,-0.7582387838826433,0.5057639451065662,-0.09336849962496865,0.6426843960989629]   |\n",
      "|[1.6981786360167042,0.08217662125700372,0.7810313053719372,-0.06458831776025438,-0.8488745229394623,-0.5665862154596442,-0.6665099473529916,0.9126042809492182,0.17424702163174322,0.42747715180197715]  |\n",
      "|[2.252433714871009,2.232556587581118,-0.109588522768928,-0.15520048092191485,-0.049645313808027204,-0.7489279879753694,-0.017378416482837774,0.7916237581149959,-0.13429338902844323,0.9382927741065717] |\n",
      "|[0.3469638105490314,-0.3336624608615506,0.7999228160594215,-0.48942286758700587,0.012519188922671724,-0.9013165697182637,-0.5250464153913033,0.6867555627035065,0.34718329802953785,0.38574698054680867] |\n",
      "|[0.06290388414815737,1.1436139983268172,-0.6452920257084019,-0.8252362510175981,-0.41682976734379723,-1.4463249905833206,-1.1551108488392028,0.5550152196601723,0.035658201354632754,0.03803581585632297]|\n",
      "|[2.5213292270463317,0.791875451027113,0.7353105586530104,-0.7739373989291413,-0.2810379972552241,-1.2879527555701071,-0.5359394141843418,0.7415022571855195,-0.07361639410811019,0.45635080497807057]    |\n",
      "|[1.0476800361386969,0.17181734966599016,0.34055808215938727,0.295636980040167,-0.02614479656728642,-1.1022470730279519,-0.7208761122675589,0.6683107708932984,0.13991764431221695,0.34689711787413186]   |\n",
      "|[0.7978722973331348,-0.4293010045546705,1.2497508972714961,-0.9174731901732991,-0.26609953351200333,-0.7502559702425495,-0.9450852329192077,0.23082589147520818,0.04063124183958286,0.5898591665002936]  |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36massembler\u001b[39m: \u001b[32mVectorAssembler\u001b[39m = vecAssembler_7bd6104fb512\n",
       "\u001b[36mfeaturesDF\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [features: vector]\n",
       "\u001b[36mpca\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mfeature\u001b[39m.\u001b[32mPCAModel\u001b[39m = pca_f627cb4ab0f6\n",
       "\u001b[36mresult\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [pcaFeatures: vector]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(crimes.columns.filterNot(name => List(\"community\", \"otherpercap\").contains(name.toLowerCase))).setOutputCol(\"features\")\n",
    "//this is different from video \n",
    "val featuresDF = assembler.transform(crimes).select(\"features\")\n",
    "\n",
    "val pca = new PCA()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"pcaFeatures\")\n",
    "  .setK(10)\n",
    "  .fit(featuresDF)\n",
    "\n",
    "val result = pca.transform(featuresDF).select(\"pcaFeatures\")\n",
    "result.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tPrincipal components are stored in a local dense matrix.\n",
    "-\tThe matrix pc is now 10 dimensions, but it represents the variability 'almost as well' as the previous 100 dimensions\n",
    "\n",
    "\n",
    "## Pros I\n",
    "\n",
    "-\tInterpretability (!)\n",
    "-\tPCA creates components that are uncorrelated, and Some predictive models prefer little to no collinearity (example linear regression)\n",
    "-\tHelps avoiding the 'curse of dimensionality': Classifiers tend to overfit the training data in high dimensional spaces, so reducing the number of dimensions may help\n",
    "\n",
    "\n",
    "## Pros II \n",
    "\n",
    "- Performance. On further modeling, the computational effort often depends on the number of variables. PCA gives you far fewer variables; this may make any further processing more performant\n",
    "-\tFor classification problems PCA can show potential separation of classes (if there is a separation).\n",
    "\n",
    "\n",
    "## Cons\n",
    "\n",
    "-\tThe computational effort often depends greatly on the number of variables and the number of data records. \n",
    "-\tPCA seeks linear combinations of predictors that maximize variability, it will naturally first be drawn to summarizing predictors that retain most of the variation in the data.\n",
    "\n",
    "\n",
    "## How Many Principal Components to Use?\n",
    "\n",
    "-\tNo simple answer to this question\n",
    "-\tBut there are heuristics:\n",
    "- find the elbow on the graph for dimensions by variance explained \n",
    "-\tSet up a 'variance explained threshold' (for example, take as many Principal components as needed to explain 95% of the variance\n",
    "\n",
    "\n",
    "\n",
    "## Tip for Best Practice\n",
    "\n",
    "-\tAlways center and scale the predictors prior to performing PCA 9see previous course). Otherwise the predictors that have more validation will soak the top principal components\n",
    "\n",
    "\n",
    "## Lesson Summary\n",
    "\n",
    "Having completed this lesson, you should be able to: \n",
    "\n",
    "-\tApply PCA in Spark\n",
    "-\tUse PCA to fix datasets with correlated predictors that could otherwise trip your models!\n",
    "\n",
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12.8",
   "language": "scala",
   "name": "scala_2_12_8"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
